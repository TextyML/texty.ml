{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Donald Trump' 'Hillary Clinton']\n",
      "[array([ 0.11111111,  0.        ]), array([ 0.08333333,  0.        ]), array([ 0.        ,  0.16666667])]\n"
     ]
    }
   ],
   "source": [
    "import nltk # language processing\n",
    "from sklearn.base import BaseEstimator\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import operator\n",
    "import telnetlib\n",
    "\n",
    "\n",
    "class NamedEntityVectorizer(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, entity=\"PERSON\", host= \"localhost\", port=\"9191\" max_features=None): #max_df=1.0, min_df=1\n",
    "        #self.max_df = max_df\n",
    "        #self.min_df = min_df\n",
    "        self.entity = entity\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.max_features = max_features\n",
    "        self.features = []\n",
    "        self._v = {}\n",
    "        self._c = {}\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        return np.array(self.features)\n",
    "\n",
    "    def _stanford_tagger(self, text):\n",
    "        #ttext = word_tokenize(text)\n",
    "        tn = telnetlib.Telnet(host=HOST, port=9191)\n",
    "        tn.write((text+\"\\n\").encode('ascii')) \n",
    "        \n",
    "        output = str(tn.read_all(), encoding=\"utf-8\")\n",
    "        tn.close()\n",
    "        output = output[:-2]\n",
    "        output = output.split(sep=\" \")\n",
    "        output = [item.split(sep=\"/\") for item in output]\n",
    "        \n",
    "        tagList = []\n",
    "        tempTag = None\n",
    "        for tag_index, tag in enumerate(output):   \n",
    "            if tag[1] != \"O\":\n",
    "                if tempTag is None:\n",
    "                    tempTag = tag\n",
    "                elif tempTag[1] == tag[1]:\n",
    "                    tempTag = (tempTag[0] + \" \"+ tag[0], tempTag[1])\n",
    "                else:\n",
    "                    tagList.append(tempTag)\n",
    "                    tempTag = tag\n",
    "            elif tempTag is not None:\n",
    "                tagList.append(tempTag)\n",
    "                tempTag = None\n",
    "\n",
    "        return(tagList)\n",
    "\n",
    "    def _count_vocab(self, raw_documents, build_doc):\n",
    "        vocabulary = {}\n",
    "        doc_counter = []\n",
    "        counter = {}\n",
    "        for doc in raw_documents:\n",
    "            tagged = self._stanford_tagger(doc)\n",
    "            list_of = [chunk for chunk in tagged if chunk[1] == self.entity]\n",
    "            list_set_of = list(set(list_of))\n",
    "            if build_doc:\n",
    "                doc_dict = {el:0.0 for el in self.features}\n",
    "                        \n",
    "            for feature in list_of:\n",
    "                try:\n",
    "                    if build_doc:\n",
    "                        doc_dict[feature[0]] += 1\n",
    "                    vocabulary[feature[0]] += 1\n",
    "                except KeyError:\n",
    "                    vocabulary[feature[0]] = 1\n",
    "                    \n",
    "            for feature in list_set_of:\n",
    "                try:\n",
    "                    counter[feature[0]] += 1\n",
    "                except KeyError:\n",
    "                    counter[feature[0]] = 1\n",
    "                    \n",
    "            if build_doc:        \n",
    "                doc_counter.append(doc_dict)\n",
    "                        \n",
    "        return vocabulary, counter, doc_counter\n",
    "        \n",
    "    def fit(self, raw_documents, y=None):\n",
    "        v, c, d = self._count_vocab(raw_documents, False)\n",
    "        if self.max_features is None or len(v) > self.max_features:\n",
    "            max = len(v)\n",
    "        sorted_v = sorted(v.items(), key=lambda x: x[1], reverse=True)[:self.max_features]\n",
    "        self.features = [p[0] for p in sorted_v]\n",
    "        self._v = v\n",
    "        self._c = c\n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        v, c, d = self._count_vocab(raw_documents, True)\n",
    "        result = np.array([list(doc.values()) for doc in d])\n",
    "        lengthes = [len(word_tokenize(document)) for document in raw_documents]\n",
    "        result_norm = [res / lengthes[ind] for ind, res in enumerate(result)]\n",
    "        return result_norm\n",
    "    \n",
    "nerv = NamedEntityVectorizer(entity=\"PERSON\",max_features=2)\n",
    "\n",
    "texts = [\"Donald Trump is made by Hillary Trump in London\", \"America's Fabian Retkowski is a person made by Donald Trump.\", \"Hillary Clinton is not a person, but Hillary Clinton is.\"]\n",
    "            \n",
    "nerv.fit(texts)\n",
    "print(nerv.get_feature_names())\n",
    "print(nerv.transform(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
