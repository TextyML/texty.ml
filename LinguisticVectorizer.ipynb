{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "dictionary = Counter(words(open('/home/retkowski/Data/dicts/dict_all.txt').read()))\n",
    "\n",
    "\n",
    "# mehr Wortarten…\n",
    "\n",
    "class LinguisticVectorizer(BaseEstimator):\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return np.array(\n",
    "            ['text_length',\n",
    "             'number_of_paragraphs',\n",
    "             'average_sent_length',\n",
    "             'average_word_length',\n",
    "             'number_of_nouns',\n",
    "             'number_of_adjectives',\n",
    "             'number_of_verbs',\n",
    "             'type_token_relation',\n",
    "             'concentration_index',\n",
    "             'hapaxes_index',\n",
    "             'action_index',\n",
    "             #'number_of_spelling_mistakes',\n",
    "             'number_of_question_marks',\n",
    "             'number_of_exclamations',\n",
    "             'number_of_percentages',\n",
    "             'number_of_currency_symbols',\n",
    "             'number_of_paragraph_symbols',\n",
    "             'content_fraction',\n",
    "             'number_of_cappsed_words',\n",
    "             'number_of_first_person_pronouns']\n",
    "        )\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "    \n",
    "    def __filter(self, string):\n",
    "        return [w for w in word_tokenize(string) if w.isalpha()]\n",
    "    \n",
    "    def _get_text_length(self, string):\n",
    "        tokens = self.__filter(string)\n",
    "        return len(tokens)\n",
    "    \n",
    "    def _get_number_of_paragraphs(self, string):\n",
    "        return round(string.count('\\n') / 2)\n",
    "    \n",
    "    def _get_average_sent_length(self, string):\n",
    "        tokens = self.__filter(string)\n",
    "        if len(sent_tokenize(string)) is 0:\n",
    "            return len(tokens)\n",
    "        return len(tokens) / len(sent_tokenize(string))\n",
    "    \n",
    "    def _get_average_word_length(self, string):\n",
    "        tokens = self.__filter(string)\n",
    "        word_length_list = []\n",
    "        for word in tokens:\n",
    "            word_length_list.append(len(word))\n",
    "        return np.average(word_length_list)\n",
    "\n",
    "    def _get_number_of_nouns(self, string):\n",
    "        nouns = [a[0] for a in pos_tag(self.__filter(string)) if a[1] in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "        return len(nouns) / self._get_text_length(string)\n",
    "    \n",
    "    def _get_number_of_adjectives(self, string):\n",
    "        adjectives = [a[0] for a in pos_tag(self.__filter(string)) if a[1] in ['JJ', 'JJR', 'JJS']]\n",
    "        return len(adjectives) / self._get_text_length(string)\n",
    "   \n",
    "    def _get_number_of_verbs(self, string):\n",
    "        verbs = [a[0] for a in pos_tag(self.__filter(string)) if a[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "        return len(verbs) / self._get_text_length(string)\n",
    "    \n",
    "    def _get_ttr(self, string):\n",
    "        tokens = self.__filter(string)\n",
    "        if len(tokens) is 0:\n",
    "            return 0\n",
    "        return len(set(tokens)) / len(tokens)\n",
    "\n",
    "    def _get_aq(self, string):\n",
    "        adjectives = self._get_number_of_adjectives(string)\n",
    "        verbs = self._get_number_of_verbs(string)\n",
    "        if adjectives is 0:\n",
    "            return verbs\n",
    "        return verbs / adjectives\n",
    "\n",
    "    def _get_naq(self, string):\n",
    "        adjectives = self._get_number_of_adjectives(string)\n",
    "        verbs = self._get_number_of_verbs(string)\n",
    "        if adjectives is 0 and verbs is 0:\n",
    "            return 0\n",
    "        return verbs / (adjectives + verbs)\n",
    "\n",
    "    def _get_hl(self, string):\n",
    "        words = self.__filter(string)\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        hapaxes = fdist.hapaxes()\n",
    "        if len(words) is 0:\n",
    "            return len(hapaxes)\n",
    "        return len(hapaxes) / len(words)\n",
    "\n",
    "    def _get_koi(self, string, n):\n",
    "        words = self.__filter(string)\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sum = 0\n",
    "        for word in fdist.most_common(n):\n",
    "            sum += word[1]\n",
    "        if len(words) is 0:\n",
    "            return sum\n",
    "        return sum / len(words)\n",
    "\n",
    "    def _get_nkoi(self, string, n, m):\n",
    "        words = self.__filter(string)\n",
    "        h = math.floor(len(words) / m)\n",
    "        if h is 0:\n",
    "            return self._get_koi(string, 15)\n",
    "        sum = 0\n",
    "        for i in range(h):\n",
    "            sum += self._get_nkoi_i(words[i*m:(i+1)*m],n)\n",
    "        return (sum/h)\n",
    "\n",
    "    def _get_nkoi_i(self, words, n):\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sum = 0\n",
    "        for word in fdist.most_common(n):\n",
    "            sum += word[1]\n",
    "        if len(words) is 0:\n",
    "            return sum\n",
    "        return sum / len(words)\n",
    "    \n",
    "    def _get_number_of_spelling_mistakes(self, string):\n",
    "        text_vocub = set(w.lower() for w in word_tokenize(string) if w.isalpha())\n",
    "        text_dict  = set(w.lower() for w in dictionary)\n",
    "        return len(text_vocub - text_dict) / self._get_text_length(string)\n",
    "    \n",
    "    def _get_number_of_currency_symbols(self, string):\n",
    "        currencies = [\"£\",\"€\",\"$\",\"¥\",\"¢\",\"₩\"]\n",
    "        sum = 0\n",
    "        for currency in currencies:\n",
    "            sum += self._get_number_of_symbol(string, currency)\n",
    "        return sum / self._get_text_length(string)\n",
    "    \n",
    "    def _get_number_of_symbol(self, string, symbol):\n",
    "        return string.count(symbol) / self._get_text_length(string)\n",
    "    \n",
    "    def _get_content_fraction(self, string):\n",
    "        tokens = self.__filter(string)\n",
    "        content = [w for w in tokens if w.lower() not in stopwords.words('english')]\n",
    "        if len(tokens) is 0:\n",
    "            return 0\n",
    "        return len(content) / len(tokens)\n",
    "    \n",
    "    def _get_number_of_cappsed_words(self, string):\n",
    "        tokens = self.__filter(string)\n",
    "        return np.sum([t.isupper() for t in tokens if len(t) > 2]) / self._get_text_length(string)\n",
    "    \n",
    "    def _get_number_of_first_person_pronouns(self, string):\n",
    "        tokens = word_tokenize(string)\n",
    "        pronouns = [\"i\",\"me\",\"my\", \"mine\", \"myself\",\"we\", \"our\", \"ours\", \"ourself\"]\n",
    "        sum = 0\n",
    "        mode = 0\n",
    "        for word in tokens:\n",
    "            if word == \"``\":\n",
    "                mode = mode + 1\n",
    "            elif word == \"''\":\n",
    "                mode = mode - 1\n",
    "            \n",
    "            if mode <= 0 and word.lower() in '\\t'.join(pronouns):\n",
    "                sum += 1\n",
    "        return sum / len(tokens)\n",
    "\n",
    "    def transform(self, documents):\n",
    "        text_length = [self._get_text_length(d) for d in documents]\n",
    "        number_of_paragraphs = [self._get_number_of_paragraphs(d) for d in documents]\n",
    "        average_length_of_sent = [self._get_average_sent_length(d) for d in documents]\n",
    "        average_word_length = [self._get_average_word_length(d) for d in documents]\n",
    "        number_of_nouns = [self._get_number_of_nouns(d) for d in documents]\n",
    "        number_of_adjectives = [self._get_number_of_adjectives(d) for d in documents]\n",
    "        number_of_verbs = [self._get_number_of_verbs(d) for d in documents]\n",
    "        type_token_relation = [self._get_ttr(d) for d in documents]\n",
    "        concentration_index = [self._get_nkoi(d,10,150) for d in documents]\n",
    "        hapaxes_index = [self._get_hl(d) for d in documents]\n",
    "        action_index = [self._get_naq(d) for d in documents]\n",
    "        #number_of_spelling_mistakes = [self._get_number_of_spelling_mistakes(d) for d in documents]\n",
    "        number_of_question_marks = [self._get_number_of_symbol(d, \"?\") for d in documents]\n",
    "        number_of_exclamations = [self._get_number_of_symbol(d, \"!\") for d in documents]\n",
    "        number_of_percentages = [self._get_number_of_symbol(d, \"%\") for d in documents]\n",
    "        number_of_currency_symbols = [self._get_number_of_currency_symbols(d) for d in documents]\n",
    "        number_of_paragraph_symbols = [self._get_number_of_symbol(d, \"§\") for d in documents]\n",
    "        content_fraction = [self._get_content_fraction(d) for d in documents]\n",
    "        number_of_cappsed_words = [self._get_number_of_cappsed_words(d) for d in documents]\n",
    "        number_of_first_person_pronouns = [self._get_number_of_first_person_pronouns(d) for d in documents]\n",
    "        \n",
    "        result = np.array(\n",
    "            [text_length,\n",
    "             number_of_paragraphs,\n",
    "             average_length_of_sent,\n",
    "             average_word_length,\n",
    "             number_of_nouns,\n",
    "             number_of_adjectives,\n",
    "             number_of_verbs,\n",
    "             type_token_relation,\n",
    "             concentration_index,\n",
    "             hapaxes_index,\n",
    "             action_index,\n",
    "             #number_of_spelling_mistakes,\n",
    "             number_of_question_marks,\n",
    "             number_of_exclamations,\n",
    "             number_of_percentages,\n",
    "             number_of_currency_symbols,\n",
    "             number_of_paragraph_symbols,\n",
    "             content_fraction,\n",
    "             number_of_cappsed_words,\n",
    "             number_of_first_person_pronouns]\n",
    "        ).T\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ling = LinguisticVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.        ,   0.        ,   4.        ,   3.125     ,\n",
       "          0.3125    ,   0.0625    ,   0.25      ,   0.75      ,\n",
       "          1.        ,   0.5625    ,   0.8       ,   0.        ,\n",
       "          0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.375     ,   0.        ,   0.04347826]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ling.transform([\"This is a test. This test has \\\"my quotes I.\\\". I am a test. tests are cool.\"])\n",
    "#word_tokenize(\"This is a test. This test has \\\"my quotes I.\\\". I am a test. tests are cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
